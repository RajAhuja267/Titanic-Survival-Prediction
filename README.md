# Titanic-Survival-Prediction
Problem Statement:
Perform the analysis of what sorts of people were likely to survive using the tools of machine learning taught during the BAP-R course.
Description:
On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This tragedy shocked the international community and lead to better safety regulations for ships. 
One of the reasons that the shipwreck lead to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. 

Data Description:
VARIABLE DESCRIPTIONS:
survival     Survival
                (0 = No; 1 = Yes)
pclass        Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
name         Name
sex             Sex
age            Age
sibsp          Number of Siblings/Spouses Aboard
parch         Number of Parents/Children Aboard
ticket         Ticket Number
fare            Passenger Fare
cabin          Cabin
embarked   Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)
      
        	
Steps for Data Pre-processing:

1.	Data Exploration:
i.	We will use R-language to do this problem.
ii.	First of all we will import all the necessary libraries and load the data and then we will check summary, structure and dimensions to explore the data and decide what to do with each variable.

2.	Variable Handling:
i.	After exploring the data we will remove the variables that are not necessary. 
ii.	Here we will remove PassengerId, Cabin, Name, Ticket these variables.
iii.	After removing the unnecessary variables we will check the data type of each variable. Then we will convert data types of variables that have character datatype.
iv.	Here we will Convert Survived, Pclass, SibSp, Parch, Sex and Embarked variables to Factor.

3.	NA’s Imputations:
i.	Now we will Check for Na’s in the data and then we will impute the Na’s with suitable values.
ii.	Here we have Na’s in Age and Embarked. Now we will impute missing values in Age with median age value that is 28 and we will impute missing value in Embarked with mode that is S.
iii.	After imputing values for Na’s we will again check that all the Na’s are removed or not.
iv.	After Na’s imputations we will divide Age variable into categories to ease the calculation. The Age variable is divides into four categories c1(age 0 to 20 years), c2(20 to 28 years), c3(28 to 40 years) and c4(40 to Infinity) according to the minimum age, median of age and maximum age.

4.	Data Visualization:
i.	After that we will do some data visualization on the data.
ii.	We will plot the graph showing the number of Survived passengers with respect to Pclass and Sex.
iii.	Then we will check the pair-plots for the entire data.
iv.	After data Visualization we will make a copy of data to apply different Machine Learning algorithms.
5.	Applying Machine Learning Algorithms:
i.	Now We will Split the data into training and testing and then we will train the model on training data and then predict the values of Survived on testing data. 
ii.	Now we will apply Binary Logistic Regression, Naïve Bayes, Random Forest and Support 
Vector Machine algorithms on the data.

Steps to perform algorithms:

For Binary Logistic Regression:
Step 1:
i.	We will apply Binary Logistic Regression to the training data and the we will values of Survived patients for test data.
Step 2:
ii.	Then we will plot the ROC (Receiver Operator Characteristic Curve).
iii.	ROC (Receiver Operator Characteristic Curve) can help in deciding the best threshold value. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis).
iv.	After that we will set the threshold for predicted probabilities in this case it is 0.35.
Step 3:
i.	After that we will generate Confusion Matrix between predicted and actual values of Survived Passengers of test data.
ii.	The performance parameters are calculated internally using below formulas.

•	Accuracy: This is the simplest scoring measure. It calculates the proportion of correctly classified instances. 
Accuracy = (TP + TN) / (TP+TN+FP+FN) 

•	Sensitivity (also called Recall or True Positive Rate): Sensitivity is the proportion of actual positives which are correctly identified as positives by the classifier. 
Sensitivity = TP / (TP +FN) 

•	Specificity (also called True Negative Rate): Specificity relates to the classifier’s ability to identify negative results. Consider the example of medical test used to identify a certain disease. The specificity of the test is the proportion of patients that do not to have the disease and will successfully test negative for it. In other words: 
Specificity: TN / (TN+FP) 

•	Precision: This is a measure of retrieved instances that are relevant. In other words: 
Precision: TP/(TP+FP)
where ;
•	True Positive (TP): Observation is positive, and is predicted to be positive.
•	False Negative (FN): Observation is positive, but is predicted negative.
•	True Negative (TN): Observation is negative, and is predicted to be negative.
•	False Positive (FP): Observation is negative, but is predicted positive.
Step 4:
i.	Now we will find the AUC value.
ii.	The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier. 
iii.	Here we got 0.8648418 which is approximately 0.9 which is very good.
Step 5:
i.	After this we will repeat the above steps for testing data and find the performance parameters and AUC values for testing data.

For Naïve Bayes, Random Forest and Support Vector Machine Algorithms:
i.	We will split the data into training and testing and then we apply the respective algorithms.
ii.	Then we will predict the values of Survived passengers on testing data using model trained on training data.
iii.	The we will print the confusion matrix and performance parameters.

Conclusion:
	From the results we can conclude that Random Forest Algorithm predict most of the cases accurately with Accuracy of approx (83.58%).
